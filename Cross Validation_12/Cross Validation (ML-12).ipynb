{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf0fc32",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "**Cross validation** is a technique used to evaluate the performance of a predictive model and assess its accuracy. It works by dividing the dataset into training and testing sets, then using the training set to build the model and the testing set to evaluate the modelâ€™s performance. This process is repeated multiple times with different combinations of training and testing sets, and the results are averaged to get an overall accuracy score. Cross validation helps ensure that the model is not overfitting or underfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ce3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries...\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d3d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split the dataset into train and test samples:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161d1e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Habib\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Logisctic Regression Classifire:\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c14f65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861111111111112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Support Vector Machine Classifier: \n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "230b5b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Random Forest Classifier:\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb42761",
   "metadata": {},
   "source": [
    "* So this is the quick result of three algorithms. As we see we split the data: 80% is using for training and 20% is using for testing. But this splitting is not uniform, if we execute it again and again it will change the samples and finally will brings changes in the models performance. So this is the key problem with train_test_split method that you can't run the model once and judge whether my model is performing better then other models. You must run it multiple times.\n",
    "* Now let's in coming part check K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746b4618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call K-Fold:\n",
    "# n_splits specify how many fold do you want?\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 3)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82129029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8] [0 1 2]\n",
      "[0 1 2 6 7 8] [3 4 5]\n",
      "[0 1 2 3 4 5] [6 7 8]\n"
     ]
    }
   ],
   "source": [
    "# Now the folds are created. The way we use this K-fold on the datasets is:\n",
    "for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40e611",
   "metadata": {},
   "source": [
    "* So as we see the sample dataset[1-9] is splited into three folds. in First iteration it used first fold [0 1 2] for testing and remaining [3 4 5 6 7 8 ] for training. in the second iteration it used [3 4 5] for testing and [0 1 2 6 7 8] for training. and similar in 3rd iteration. \n",
    "* So now we use K-fold for our Digits example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbbb45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simplify the things, we create a generic method which takes a model, X_train, X_test, y_train and y_test as input.\n",
    "# And then train the corresponding model.\n",
    "# Once the model is trained, the function will return the model score.\n",
    "def get_score (model, X_train, X_test, y_train, t_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d9cb002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9694444444444444"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use this method to check the performance of upper three algorithms. We can just pass different model and the \n",
    "# function will return the relevant model scores:\n",
    "score = get_score(RandomForestClassifier(), X_train, X_test, y_train, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd30ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So once we have this method, we want to apply K-Fold on our Digits dataset. Here we used StratifiedKFold. It's similar to \n",
    "# KFold but it's a little better in a way that when you're seperating your folds it will divide each of the classification \n",
    "# category in a uniform way. And this property could be very helpful.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e390d416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Habib\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Habib\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Habib\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# So once we have the folds ready, we prepare the scores array to prepare the scores of different models:\n",
    "scores_lg = []\n",
    "scores_svm = []\n",
    "scores_rf = []\n",
    "\n",
    "for train_index, test_index in kf.split(digits.data):\n",
    "    X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index], \\\n",
    "                                       digits.target[train_index], digits.target[test_index]\n",
    "\n",
    "        # Now to measure the performance of the three models in each iteration, so as we have three folds, this loop will\n",
    "        # iterate three times. Every time it will takes the X_train, X_test, y_train and y_test and measure the performance\n",
    "        # of the models and then we'll append the scores in these arrays.\n",
    "        \n",
    "    scores_lg.append(get_score(LogisticRegression(), X_train, X_test, y_train, y_test))\n",
    "    scores_svm.append(get_score(SVC(), X_train, X_test, y_train, y_test))\n",
    "    scores_rf.append(get_score( RandomForestClassifier(), X_train, X_test, y_train, y_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e682aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9232053422370617, 0.9415692821368948, 0.9148580968280468]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Scores:\n",
    "scores_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7820f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9666110183639399, 0.9816360601001669, 0.9549248747913188]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machine Scores:\n",
    "scores_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf1c02ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9298831385642737, 0.9649415692821369, 0.9215358931552587]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Scores:\n",
    "scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0959a86",
   "metadata": {},
   "source": [
    "* As result we can average the score and decide which model is performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc4e37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of writting the above codes, sklearn provide us a method called cross_val_score to do the exact things that we\n",
    "# did in [29]. The upper code was just for the concept understanding purpose. When you doing ML, you don't need to write \n",
    "# that much code.\n",
    "# Let's see here the method:\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61b28c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93333333, 0.91388889, 0.94428969, 0.97214485, 0.91922006])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The method takes three arguments: 1) Model 2) Your 'X' 3) You 'Y' or 'target':\n",
    "cross_val_score(RandomForestClassifier(n_estimators = 100), digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af02ded",
   "metadata": {},
   "source": [
    "* When we try different parameters for model better performance, it's called **parameter tuning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "916b92ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96111111, 0.94444444, 0.98328691, 0.98885794, 0.93871866])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For SVM:\n",
    "cross_val_score(SVC(), digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195c444",
   "metadata": {},
   "source": [
    "* So, as we see Cross Validation technique is very useful. It don't only allow you to compare different algorithms but it also allow you to compare the same algorithm with different parameters how it would increase the performance.\n",
    "* So ML is not like scientific question, where for a given problem you should use this model or that model. No it's try and error based, for a given problem and given dataset you need to try various models with various parameters and then figure out which one is the best for your usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise\n",
    "Use iris flower dataset from sklearn library and use cross_val_score against following models to measure the performance of each. In the end figure out the model with best performance,\n",
    "\n",
    "    1. Logistic Regression\n",
    "    2. SVM\n",
    "   3. Decision Tree\n",
    "4. Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
